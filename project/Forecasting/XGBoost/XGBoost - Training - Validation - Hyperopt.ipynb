{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the python path to the folder containing some useful custom packages.\n",
    "import sys\n",
    "sys.path.insert(0, \"../../packages/\")\n",
    "from TsIP.TsIP import TsIP\n",
    "from tools import find_multiple_sets\n",
    "from LagsCreator.LagsCreator import LagsCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workspace.\n",
    "dir = \"./output\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)           \n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = \"../../Dataset time-series/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the training sets.\n",
    "train = pd.read_csv(PATH_TO_DATA_FOLDER + \"train_smooth.csv\", header = [0, 1], index_col = 0)\n",
    "train.index.name = \"Datetime\"\n",
    "train.index = pd.to_datetime(train.index)\n",
    "freq = \"D\"\n",
    "train.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the test sets.\n",
    "test = pd.read_csv(PATH_TO_DATA_FOLDER + \"test_target.csv\", header = [0, 1], index_col = 0)\n",
    "test.index.name = \"Datetime\"\n",
    "test.index = pd.to_datetime(test.index)\n",
    "freq = \"D\"\n",
    "test.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the whole time-series of the fcs indicator.\n",
    "target = pd.read_csv(PATH_TO_DATA_FOLDER + \"all_target.csv\", header = [0, 1], index_col = 0)\n",
    "target.index.name = \"Datetime\"\n",
    "target.index = pd.to_datetime(target.index)\n",
    "freq = \"D\"\n",
    "target.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 30\n",
    "FREQ = train.index.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Abyan', 'Aden', 'Al Bayda', 'Al Dhale'e', 'Al Hudaydah', 'Al Jawf',\n",
       "       'Al Maharah', 'Al Mahwit', 'Amanat Al Asimah', 'Amran', 'Dhamar',\n",
       "       'Hajjah', 'Ibb', 'Lahj', 'Marib', 'Raymah', 'Sa'ada', 'Sana'a',\n",
       "       'Shabwah', 'Taizz'],\n",
       "      dtype='object', name='AdminStrata')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROVINCES = TRAIN.columns.get_level_values(0).unique()\n",
    "PROVINCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1 Month Anomaly (%) Rainfall', '3 Months Anomaly (%) Rainfall',\n",
       "       'Cereals and tubers', 'Exchange rate (USD/LCU)', 'FCS', 'Fatality',\n",
       "       'Lat', 'Lon', 'NDVI Anomaly', 'Population', 'Rainfall (mm)', 'Ramadan',\n",
       "       'rCSI'],\n",
       "      dtype='object', name='Indicator')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTORS = TRAIN.columns.get_level_values(1).unique()\n",
    "PREDICTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets.\n",
    "TRAIN_NORMALIZED_SETS = find_multiple_sets(train)\n",
    "TEST_TARGET_SETS = find_multiple_sets(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation\n",
    "### Parameters grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the PARAMETERS MODEL to which perform the grid search.\n",
    "space = {\"1 Month Anomaly (%) Rainfall\": hp.randint(\"1 Month Anomaly (%) Rainfall\", 1, 5), \n",
    "         \"3 Months Anomaly (%) Rainfall\": hp.randint(\"3 Months Anomaly (%) Rainfall\", 1, 5), \n",
    "         \"Cereals and tubers\": hp.randint(\"Cereals and tubers\", 1, 5), \n",
    "         \"Exchange rate (USD/LCU)\": hp.randint(\"Exchange rate (USD/LCU)\", 1, 5), \n",
    "         \"FCS\": hp.randint(\"FCS\", 1, 5), \n",
    "         \"Fatality\": hp.randint(\"Fatality\", 1, 5), \n",
    "         \"NDVI Anomaly\": hp.randint(\"NDVI Anomaly\", 1, 5), \n",
    "         \"Rainfall (mm)\": hp.randint(\"Rainfall (mm)\", 1, 5), \n",
    "         \"rCSI\": hp.randint(\"rCSI\", 1, 5), \n",
    "         \"Lat\": hp.randint(\"Lat\", 0, 1), \n",
    "         \"Lon\": hp.randint(\"Lon\", 0, 1), \n",
    "         \"Population\": hp.randint(\"Population\", 0, 1), \n",
    "         \"Ramadan\": hp.randint(\"Ramadan\", 1, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters(space):  \n",
    "    try:\n",
    "        val_losses_h = list()\n",
    "        for h in range(TEST_SIZE):\n",
    "            X_train_list, y_train_list, X_val_list, y_val_list = list(), list(), list(), list()\n",
    "            for train_normalized in TRAIN_NORMALIZED_SETS:\n",
    "                # Create training and validation samples.  \n",
    "                for PROVINCE in PROVINCES:\n",
    "                    creator = LagsCreator(train_normalized[[PROVINCE]], lags_dictionary = space, target = \"FCS\")\n",
    "                    X_train, y_train, X_val, y_val, _ = creator.to_supervised(n_out = TEST_SIZE, single_step = True, h = h+1, return_dataframe = True,\n",
    "                                                                              feature_time = True, validation = True, return_single_level = True, \n",
    "                                                                              dtype = np.float32)\n",
    "                    X_train_list.append(X_train)\n",
    "                    y_train_list.append(y_train)\n",
    "                    X_val_list.append(X_val)\n",
    "                    y_val_list.append(y_val)  \n",
    "\n",
    "            X_train = pd.concat(X_train_list).reset_index(drop = True)\n",
    "            y_train = pd.concat(y_train_list).reset_index(drop = True)\n",
    "\n",
    "            # Train the model.\n",
    "            print(\"Training %s samples for the prediction horizon h: %d\" % (str(X_train.shape), h+1), end = \"\\r\")\n",
    "            model = xgb.XGBRegressor(objective = \"reg:squarederror\", n_estimators = 100)   #tree_method = \"gpu_hist\", gpu_id = 0\n",
    "            model.fit(X_train, y_train)  \n",
    "\n",
    "            y_hats_train = model.predict(X_train)\n",
    "            # Compute training error.\n",
    "            train_loss = mean_squared_error(y_train.values.flatten(), y_hats_train)\n",
    "\n",
    "            X_val = pd.concat(X_val_list).reset_index(drop = True)\n",
    "            y_val = pd.concat(y_val_list).reset_index(drop = True)\n",
    "\n",
    "            # Validation.\n",
    "            y_hats_val = model.predict(X_val)\n",
    "            # Compute validation error.\n",
    "            val_loss = mean_squared_error(y_val.values.flatten(), y_hats_val)\n",
    "            val_losses_h.append(val_loss)\n",
    "            \n",
    "            # Recursive save results.\n",
    "            results = space.copy()\n",
    "            results[\"h\"] = h+1\n",
    "            results[\"val_loss\"] = val_loss\n",
    "            results[\"train_loss\"] = train_loss\n",
    "            df_space = pd.DataFrame(results, index = [0], dtype = object)\n",
    "            filename = dir + \"/grid_search.csv\"\n",
    "            df_space.to_csv(filename, index = False, header = (not os.path.exists(filename)), mode = \"a\")\n",
    "            clear_output(wait = True)\n",
    "\n",
    "        # Compute mean error of this 'space' for the various prediction horizions.\n",
    "        val_loss = np.mean(val_losses_h)\n",
    "    except:\n",
    "        val_loss = np.inf     \n",
    "        clear_output(wait = True)\n",
    "\n",
    "    return {\"loss\": val_loss, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 3/3 [00:30<00:00, 10.07s/trial, best loss: inf]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = hyperparameters,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 3,\n",
    "            trials = trials)\n",
    "\n",
    "# Save the trials into a file.\n",
    "pickle.dump(trials, open(dir + \"/hyp_trials.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
