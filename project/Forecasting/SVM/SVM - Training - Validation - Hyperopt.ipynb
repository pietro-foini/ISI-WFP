{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the python path to the folder containing some useful custom packages.\n",
    "import sys\n",
    "sys.path.insert(0, \"../../packages/\")\n",
    "from TsIP.TsIP import TsIP\n",
    "from tools import find_multiple_sets\n",
    "from LagsCreator.LagsCreator import LagsCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workspace.\n",
    "dir = \"./output\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)           \n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = \"Yemen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = \"../../Dataset time-series/data/\" + COUNTRY + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the smoothed training sets.\n",
    "train_smooth = pd.read_csv(PATH_TO_DATA_FOLDER + \"train_smooth.csv\", header = [0, 1], index_col = 0)\n",
    "train_smooth.index.name = \"Datetime\"\n",
    "train_smooth.index = pd.to_datetime(train_smooth.index)\n",
    "freq = \"D\"\n",
    "train_smooth.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the test sets.\n",
    "test = pd.read_csv(PATH_TO_DATA_FOLDER + \"test_target.csv\", header = [0, 1], index_col = 0)\n",
    "test.index.name = \"Datetime\"\n",
    "test.index = pd.to_datetime(test.index)\n",
    "freq = \"D\"\n",
    "test.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the whole time-series of the fcs indicator.\n",
    "target = pd.read_csv(PATH_TO_DATA_FOLDER + \"all_target.csv\", header = [0, 1], index_col = 0)\n",
    "target.index.name = \"Datetime\"\n",
    "target.index = pd.to_datetime(target.index)\n",
    "freq = \"D\"\n",
    "target.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = train_smooth.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 30\n",
    "FREQ = TRAIN.index.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Abyan', 'Aden', 'Al Bayda', 'Al Dhale'e', 'Al Hudaydah', 'Al Jawf',\n",
       "       'Al Maharah', 'Al Mahwit', 'Amanat Al Asimah', 'Amran', 'Dhamar',\n",
       "       'Hajjah', 'Ibb', 'Lahj', 'Marib', 'Raymah', 'Sa'ada', 'Sana'a',\n",
       "       'Shabwah', 'Taizz'],\n",
       "      dtype='object', name='AdminStrata')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROVINCES = TRAIN.columns.get_level_values(0).unique()\n",
    "PROVINCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1 Month Anomaly (%) Rainfall', '3 Months Anomaly (%) Rainfall',\n",
       "       'Cereals and tubers', 'Exchange rate (USD/LCU)', 'FCS', 'Fatality',\n",
       "       'Lat', 'Lon', 'NDVI Anomaly', 'Population', 'Rainfall (mm)', 'Ramadan',\n",
       "       'rCSI'],\n",
       "      dtype='object', name='Indicator')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTORS = TRAIN.columns.get_level_values(1).unique()\n",
    "PREDICTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source transformation\n",
    "\n",
    "I decide to normalize the data among the provinces considering indicator by indicator and considering only the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "global SCALERS\n",
    "\n",
    "MIN = 0\n",
    "MAX = 1\n",
    "SCALERS = dict()\n",
    "def normalization(group, feature_range):\n",
    "    min_, max_ = feature_range\n",
    "    min_group = group.min().min()\n",
    "    max_group = group.max().max()\n",
    "    \n",
    "    # Normalization.\n",
    "    group_std = (group - min_group) / (max_group - min_group)\n",
    "    group_scaled = group_std * (max_ - min_) + min_\n",
    "\n",
    "    # Save the scalers for the various indicators.\n",
    "    SCALERS[group.name] = (min_group, max_group)\n",
    "\n",
    "    return group_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NORMALIZED = TRAIN.groupby(axis = 1, level = 1).apply(lambda x: normalization(x, (MIN, MAX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time-series.\n",
    "#TsIP(TRAIN_NORMALIZED).interactive_plot_df(title = \"Training sets\", matplotlib = False, style = \"lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalization(group_scaled, indicator, feature_range, scalers):\n",
    "    min_, max_ = feature_range\n",
    "    min_group, max_group = scalers[indicator]\n",
    "\n",
    "    group_std = (group_scaled - min_) / (max_ - min_)\n",
    "    group = (group_std * (max_group - min_group)) + min_group\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets.\n",
    "TRAIN_NORMALIZED_SETS = find_multiple_sets(TRAIN_NORMALIZED)\n",
    "TEST_TARGET_SETS = find_multiple_sets(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation\n",
    "### Parameters grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LAGS to which perform the grid search.\n",
    "space1 = {\"1 Month Anomaly (%) Rainfall\": hp.choice(\"1 Month Anomaly (%) Rainfall\", np.append(np.arange(1, 10, 5), None)), \n",
    "          \"3 Months Anomaly (%) Rainfall\": hp.choice(\"3 Months Anomaly (%) Rainfall\", np.append(np.arange(1, 10, 5), None)), \n",
    "          \"Cereals and tubers\": hp.choice(\"Cereals and tubers\", np.append(np.arange(1, 10, 5), None)), \n",
    "          \"Exchange rate (USD/LCU)\": hp.choice(\"Exchange rate (USD/LCU)\", np.append(np.arange(1, 10, 5), None)), \n",
    "          \"FCS\": hp.choice(\"FCS\", np.arange(1, 10, 5)), \n",
    "          \"Fatality\": hp.choice(\"Fatality\", np.append(np.arange(1, 10, 5), None)), \n",
    "          \"NDVI Anomaly\": hp.choice(\"NDVI Anomaly\", np.append(np.arange(1, 10, 5), None)), \n",
    "          \"Rainfall (mm)\": hp.choice(\"Rainfall (mm)\", np.append(np.arange(1, 10, 5), None)), \n",
    "          \"rCSI\": hp.choice(\"rCSI\", np.append(np.arange(1, 10, 5), None)), \n",
    "          \"Lat\": hp.choice(\"Lat\", np.append(np.arange(0, 1), None)), \n",
    "          \"Lon\": hp.choice(\"Lon\", np.append(np.arange(0, 1), None)), \n",
    "          \"Population\": hp.choice(\"Population\", np.append(np.arange(0, 1), None)), \n",
    "          \"Ramadan\": hp.choice(\"Ramadan\", np.append(np.arange(1, 10, 5), None))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PARAMETERS MODEL to which perform the grid search.\n",
    "space2 = {\"C\": hp.choice(\"C\", [0.1, 1, 10, 100, 1000]),  \n",
    "          \"gamma\": hp.choice(\"gamma\", [1, 0.1, 0.01, 0.001, 0.0001])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dictionary to perform the grid search.\n",
    "space = dict(space1, **space2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters(space): \n",
    "    #try:\n",
    "    print(space)\n",
    "    # Select lags.\n",
    "    lags_dict = {key: space[key] for key in PREDICTORS}\n",
    "  \n",
    "    val_losses_h = list()\n",
    "    for h in range(TEST_SIZE):\n",
    "        X_train_list, y_train_list, X_val_list, y_val_list = list(), list(), list(), list()\n",
    "        for train_normalized in TRAIN_NORMALIZED_SETS:\n",
    "            # Create training and validation samples.  \n",
    "            for PROVINCE in PROVINCES:\n",
    "                creator = LagsCreator(train_normalized[[PROVINCE]], lags_dictionary = lags_dict, target = \"FCS\")\n",
    "                X_train, y_train, X_val, y_val, _ = creator.to_supervised(n_out = TEST_SIZE, single_step = True, h = h+1, return_dataframe = True,\n",
    "                                                                          feature_time = False, validation = True, return_single_level = True, \n",
    "                                                                          dtype = np.float64)\n",
    "                X_train_list.append(X_train)\n",
    "                y_train_list.append(y_train)\n",
    "                X_val_list.append(X_val)\n",
    "                y_val_list.append(y_val)  \n",
    "\n",
    "        X_train = pd.concat(X_train_list).reset_index(drop = True).values\n",
    "        y_train = pd.concat(y_train_list).reset_index(drop = True).values.flatten()\n",
    "\n",
    "        # Train the model.\n",
    "        print(\"Training %s samples for the prediction horizon h: %d\" % (str(X_train.shape), h+1))\n",
    "        model = SVR(gamma = space[\"gamma\"], kernel = \"rbf\", C = space[\"C\"])\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_hats_train = model.predict(X_train)\n",
    "        # Compute training error.\n",
    "        train_loss = mean_squared_error(y_train, y_hats_train)\n",
    "        r2 = model.score(X_train, y_train)\n",
    "\n",
    "        X_val = pd.concat(X_val_list).reset_index(drop = True).values\n",
    "        y_val = pd.concat(y_val_list).reset_index(drop = True).values.flatten()\n",
    "\n",
    "        # Validation.\n",
    "        y_hats_val = model.predict(X_val)\n",
    "        # Compute validation error.\n",
    "        val_loss = mean_squared_error(y_val, y_hats_val)\n",
    "        val_losses_h.append(val_loss)\n",
    "\n",
    "        # Recursive save results.\n",
    "        results = space.copy()\n",
    "        results[\"h\"] = h+1\n",
    "        results[\"r2\"] = r2\n",
    "        results[\"val_loss\"] = val_loss\n",
    "        results[\"train_loss\"] = train_loss\n",
    "        df_space = pd.DataFrame(results, index = [0], dtype = object)\n",
    "        filename = dir + \"/grid_search.csv\"\n",
    "        df_space.to_csv(filename, index = False, header = (not os.path.exists(filename)), mode = \"a\")\n",
    "    # Compute mean error of this 'space' for the various prediction horizions.\n",
    "    val_loss = np.mean(val_losses_h)\n",
    "    #except:\n",
    "    #    val_loss = np.inf     \n",
    "\n",
    "    return {\"loss\": val_loss, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1 Month Anomaly (%) Rainfall': 6, '3 Months Anomaly (%) Rainfall': 6, 'C': 1, 'Cereals and tubers': 6, 'Exchange rate (USD/LCU)': None, 'FCS': 1, 'Fatality': None, 'Lat': 0, 'Lon': None, 'NDVI Anomaly': None, 'Population': None, 'Rainfall (mm)': 6, 'Ramadan': None, 'gamma': 1, 'rCSI': 1}\n",
      "Training (8400, 27) samples for the prediction horizon h: 1                                                            \n",
      "  0%|                                                                            | 0/1 [00:02<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: name 'deed' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/1 [00:02<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'deed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-0132368a9610>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mmax_evals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             trials = trials)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Save the trials into a file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m         )\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m         )\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    284\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             )\n\u001b[1;32m--> 894\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-a611c452553e>\u001b[0m in \u001b[0;36mhyperparameters\u001b[1;34m(space)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/grid_search.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mdf_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mdeed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;31m# Compute mean error of this 'space' for the various prediction horizions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_losses_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deed' is not defined"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = hyperparameters,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 1,\n",
    "            trials = trials)\n",
    "\n",
    "# Save the trials into a file.\n",
    "pickle.dump(trials, open(dir + \"/hyp_trials.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
