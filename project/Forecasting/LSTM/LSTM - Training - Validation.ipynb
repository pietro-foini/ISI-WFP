{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from tqdm.keras import TqdmCallback\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the python path to the folder containing some useful custom packages.\n",
    "import sys\n",
    "sys.path.insert(0, \"../../packages/\")\n",
    "from TsIP.TsIP import TsIP\n",
    "from tools import find_multiple_sets\n",
    "from LagsCreator.LagsCreator import LagsCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = \"Yemen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = \"../../Dataset time-series/data/\" + COUNTRY + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the smoothed training sets.\n",
    "train_smooth = pd.read_csv(PATH_TO_DATA_FOLDER + \"train_smooth.csv\", header = [0, 1], index_col = 0)\n",
    "train_smooth.index.name = \"Datetime\"\n",
    "train_smooth.index = pd.to_datetime(train_smooth.index)\n",
    "freq = \"D\"\n",
    "train_smooth.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the test sets.\n",
    "test = pd.read_csv(PATH_TO_DATA_FOLDER + \"test_target.csv\", header = [0, 1], index_col = 0)\n",
    "test.index.name = \"Datetime\"\n",
    "test.index = pd.to_datetime(test.index)\n",
    "freq = \"D\"\n",
    "test.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the whole time-series of the fcs indicator.\n",
    "target = pd.read_csv(PATH_TO_DATA_FOLDER + \"all_target.csv\", header = [0, 1], index_col = 0)\n",
    "target.index.name = \"Datetime\"\n",
    "target.index = pd.to_datetime(target.index)\n",
    "freq = \"D\"\n",
    "target.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = train_smooth.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 30\n",
    "FREQ = TRAIN.index.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Abyan', 'Aden', 'Al Bayda', 'Al Dhale'e', 'Al Hudaydah', 'Al Jawf',\n",
       "       'Al Maharah', 'Al Mahwit', 'Amanat Al Asimah', 'Amran', 'Dhamar',\n",
       "       'Hajjah', 'Ibb', 'Lahj', 'Marib', 'Raymah', 'Sa'ada', 'Sana'a',\n",
       "       'Shabwah', 'Taizz'],\n",
       "      dtype='object', name='AdminStrata')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROVINCES = TRAIN.columns.get_level_values(0).unique()\n",
    "PROVINCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1 Month Anomaly (%) Rainfall', '3 Months Anomaly (%) Rainfall',\n",
       "       'Cereals and tubers', 'Exchange rate (USD/LCU)', 'FCS', 'Fatality',\n",
       "       'Lat', 'Lon', 'NDVI Anomaly', 'Population', 'Rainfall (mm)', 'Ramadan',\n",
       "       'rCSI'],\n",
       "      dtype='object', name='Indicator')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTORS = TRAIN.columns.get_level_values(1).unique()\n",
    "PREDICTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source transformation\n",
    "\n",
    "I decide to normalize the data among the provinces considering indicator by indicator of the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global SCALERS\n",
    "\n",
    "MIN = 0\n",
    "MAX = 1\n",
    "SCALERS = dict()\n",
    "def normalization(group, feature_range):\n",
    "    min_, max_ = feature_range\n",
    "    min_group = group.min().min()\n",
    "    max_group = group.max().max()\n",
    "    \n",
    "    # Normalization.\n",
    "    group_std = (group - min_group) / (max_group - min_group)\n",
    "    group_scaled = group_std * (max_ - min_) + min_\n",
    "\n",
    "    # Save the scalers for the various indicators.\n",
    "    SCALERS[group.name] = (min_group, max_group)\n",
    "\n",
    "    return group_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>AdminStrata</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Abyan</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Taizz</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indicator</th>\n",
       "      <th>1 Month Anomaly (%) Rainfall</th>\n",
       "      <th>3 Months Anomaly (%) Rainfall</th>\n",
       "      <th>Cereals and tubers</th>\n",
       "      <th>Exchange rate (USD/LCU)</th>\n",
       "      <th>FCS</th>\n",
       "      <th>Fatality</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>NDVI Anomaly</th>\n",
       "      <th>Population</th>\n",
       "      <th>...</th>\n",
       "      <th>Exchange rate (USD/LCU)</th>\n",
       "      <th>FCS</th>\n",
       "      <th>Fatality</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>NDVI Anomaly</th>\n",
       "      <th>Population</th>\n",
       "      <th>Rainfall (mm)</th>\n",
       "      <th>Ramadan</th>\n",
       "      <th>rCSI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-22</th>\n",
       "      <td>0.202614</td>\n",
       "      <td>0.327464</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>0.097113</td>\n",
       "      <td>0.398119</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.203677</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100691</td>\n",
       "      <td>0.521481</td>\n",
       "      <td>0.124455</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.317287</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.430215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-23</th>\n",
       "      <td>0.201694</td>\n",
       "      <td>0.323172</td>\n",
       "      <td>0.115215</td>\n",
       "      <td>0.108255</td>\n",
       "      <td>0.416745</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.201767</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105181</td>\n",
       "      <td>0.505624</td>\n",
       "      <td>0.118668</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.314277</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.436639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.513277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-24</th>\n",
       "      <td>0.200218</td>\n",
       "      <td>0.317813</td>\n",
       "      <td>0.118301</td>\n",
       "      <td>0.112477</td>\n",
       "      <td>0.422543</td>\n",
       "      <td>0.014297</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.199870</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109271</td>\n",
       "      <td>0.494372</td>\n",
       "      <td>0.115789</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.311164</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.446466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-25</th>\n",
       "      <td>0.198491</td>\n",
       "      <td>0.312132</td>\n",
       "      <td>0.119918</td>\n",
       "      <td>0.113002</td>\n",
       "      <td>0.424258</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.197950</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113148</td>\n",
       "      <td>0.486382</td>\n",
       "      <td>0.115893</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.307901</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.458051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-26</th>\n",
       "      <td>0.196724</td>\n",
       "      <td>0.306528</td>\n",
       "      <td>0.121119</td>\n",
       "      <td>0.112481</td>\n",
       "      <td>0.427209</td>\n",
       "      <td>0.016327</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.195990</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116965</td>\n",
       "      <td>0.480822</td>\n",
       "      <td>0.118913</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.304470</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.470108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.512072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "AdminStrata                        Abyan                                \\\n",
       "Indicator   1 Month Anomaly (%) Rainfall 3 Months Anomaly (%) Rainfall   \n",
       "Datetime                                                                 \n",
       "2018-08-22                      0.202614                      0.327464   \n",
       "2018-08-23                      0.201694                      0.323172   \n",
       "2018-08-24                      0.200218                      0.317813   \n",
       "2018-08-25                      0.198491                      0.312132   \n",
       "2018-08-26                      0.196724                      0.306528   \n",
       "\n",
       "AdminStrata                                                                 \\\n",
       "Indicator   Cereals and tubers Exchange rate (USD/LCU)       FCS  Fatality   \n",
       "Datetime                                                                     \n",
       "2018-08-22            0.109379                0.097113  0.398119  0.016014   \n",
       "2018-08-23            0.115215                0.108255  0.416745  0.014270   \n",
       "2018-08-24            0.118301                0.112477  0.422543  0.014297   \n",
       "2018-08-25            0.119918                0.113002  0.424258  0.015185   \n",
       "2018-08-26            0.121119                0.112481  0.427209  0.016327   \n",
       "\n",
       "AdminStrata                                              ...  \\\n",
       "Indicator         Lat       Lon NDVI Anomaly Population  ...   \n",
       "Datetime                                                 ...   \n",
       "2018-08-22   0.204339  0.354998     0.203677   0.137715  ...   \n",
       "2018-08-23   0.204339  0.354998     0.201767   0.137715  ...   \n",
       "2018-08-24   0.204339  0.354998     0.199870   0.137715  ...   \n",
       "2018-08-25   0.204339  0.354998     0.197950   0.137715  ...   \n",
       "2018-08-26   0.204339  0.354998     0.195990   0.137715  ...   \n",
       "\n",
       "AdminStrata                   Taizz                                        \\\n",
       "Indicator   Exchange rate (USD/LCU)       FCS  Fatality      Lat      Lon   \n",
       "Datetime                                                                    \n",
       "2018-08-22                 0.100691  0.521481  0.124455  0.13656  0.07253   \n",
       "2018-08-23                 0.105181  0.505624  0.118668  0.13656  0.07253   \n",
       "2018-08-24                 0.109271  0.494372  0.115789  0.13656  0.07253   \n",
       "2018-08-25                 0.113148  0.486382  0.115893  0.13656  0.07253   \n",
       "2018-08-26                 0.116965  0.480822  0.118913  0.13656  0.07253   \n",
       "\n",
       "AdminStrata                                                          \n",
       "Indicator   NDVI Anomaly Population Rainfall (mm) Ramadan      rCSI  \n",
       "Datetime                                                             \n",
       "2018-08-22      0.317287   0.894478      0.430215     0.0  0.522625  \n",
       "2018-08-23      0.314277   0.894478      0.436639     0.0  0.513277  \n",
       "2018-08-24      0.311164   0.894478      0.446466     0.0  0.511028  \n",
       "2018-08-25      0.307901   0.894478      0.458051     0.0  0.511425  \n",
       "2018-08-26      0.304470   0.894478      0.470108     0.0  0.512072  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_NORMALIZED = TRAIN.groupby(axis = 1, level = 1).apply(lambda x: normalization(x, (MIN, MAX)))\n",
    "TRAIN_NORMALIZED.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time-series.\n",
    "#TsIP(TRAIN_NORMALIZED).interactive_plot_df(title = \"Training sets\", matplotlib = False, style = \"lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalization(group_scaled, indicator, feature_range, scalers):\n",
    "    min_, max_ = feature_range\n",
    "    min_group, max_group = scalers[indicator]\n",
    "\n",
    "    group_std = (group_scaled - min_) / (max_ - min_)\n",
    "    group = (group_std * (max_group - min_group)) + min_group\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets.\n",
    "TRAIN_NORMALIZED_SETS = find_multiple_sets(TRAIN_NORMALIZED)\n",
    "TEST_TARGET_SETS = find_multiple_sets(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAGS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_dict = dict()\n",
    "# Define lags for each indicator.\n",
    "lags_dict[\"3 Months Anomaly (%) Rainfall\"] = LAGS\n",
    "lags_dict[\"1 Month Anomaly (%) Rainfall\"] = LAGS\n",
    "lags_dict[\"Cereals and tubers\"] = LAGS\n",
    "lags_dict[\"Exchange rate (USD/LCU)\"] = LAGS\n",
    "lags_dict[\"FCS\"] = LAGS\n",
    "lags_dict[\"Fatality\"] = LAGS\n",
    "lags_dict[\"NDVI Anomaly\"] = LAGS\n",
    "lags_dict[\"Rainfall (mm)\"] = LAGS\n",
    "lags_dict[\"rCSI\"] = LAGS\n",
    "lags_dict[\"Lat\"] = LAGS\n",
    "lags_dict[\"Lon\"] = LAGS\n",
    "lags_dict[\"Population\"] = LAGS\n",
    "lags_dict[\"Ramadan\"] = LAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: X: (6120, 15, 13) y: (6120, 30)\n",
      "Validation shape: X: (60, 15, 13) y: (60, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train_list, y_train_list, X_val_list, y_val_list = list(), list(), list(), list()\n",
    "# Create training and validation points starting from the training sets.\n",
    "for train_normalized in TRAIN_NORMALIZED_SETS:\n",
    "    # Create training points and validation points from the training set.\n",
    "    for PROVINCE in PROVINCES:\n",
    "        # Initialize lags creator.\n",
    "        creator = LagsCreator(train_normalized[[PROVINCE]], lags_dictionary = lags_dict, n_out = TEST_SIZE, target = \"FCS\", \n",
    "                              return_dataframe = False)\n",
    "        # Get samples.\n",
    "        X_train, y_train, X_val, y_val, _ = creator.to_supervised(single_step = False, feature_time = False, validation = True, \n",
    "                                                                  dtype = np.float64)\n",
    "    \n",
    "        # Add a list of all the training and validation samples of all the provinces together.\n",
    "        X_train_list.append(X_train)\n",
    "        y_train_list.append(y_train)\n",
    "        X_val_list.append(X_val)\n",
    "        y_val_list.append(y_val)\n",
    "\n",
    "X_train = np.concatenate(X_train_list)\n",
    "y_train = np.concatenate(y_train_list)\n",
    "X_val = np.concatenate(X_val_list)\n",
    "y_val = np.concatenate(y_val_list)\n",
    "\n",
    "print(\"Training shape: X:\", X_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Validation shape: X:\", X_val.shape, \"y:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(timesteps, features, n_out):      \n",
    "    model = Sequential()\n",
    " \n",
    "    # MODEL.\n",
    "    model.add(LSTM(128, activation = \"relu\", return_sequences = True, batch_input_shape = (None, timesteps, features)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(32, activation = \"relu\", return_sequences = False, batch_input_shape = (None, timesteps, features)))\n",
    "    model.add(Dropout(0.1))  \n",
    "\n",
    "    model.add(Dense(n_out))  \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6ee8cbb2b64f7c8d25b435890f8a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843b759c4e314c60a3541e5c85acb5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bad716435d05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Fit model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m history = model.fit(X_train, y_train, epochs = N_EPOCHS, validation_data = (X_val, y_val), batch_size = BATCH_SIZE, \n\u001b[1;32m---> 12\u001b[1;33m                     verbose = 0, shuffle = True, callbacks = [es, TqdmCallback(verbose = 1)])\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Plot result of the training and validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training network:\")\n",
    "\n",
    "# Model.\n",
    "model = network(LAGS, N_FEATURES, TEST_SIZE)\n",
    "# Compile model.\n",
    "model.compile(loss = \"mse\", optimizer = \"adam\")\n",
    "\n",
    "# Patient early stopping.\n",
    "es = EarlyStopping(monitor = \"val_loss\", mode = \"min\", verbose = 1, patience = 200)\n",
    "# Fit model.\n",
    "history = model.fit(X_train, y_train, epochs = N_EPOCHS, validation_data = (X_val, y_val), batch_size = BATCH_SIZE, \n",
    "                    verbose = 0, shuffle = True, callbacks = [es, TqdmCallback(verbose = 1)])\n",
    "\n",
    "# Plot result of the training and validation.\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc = \"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example predict on sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ad190fe81726>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mid_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Train sample.\n",
    "\n",
    "id_ = 30\n",
    "\n",
    "X_test = np.expand_dims(X_train[id_], axis = 0)\n",
    "\n",
    "pred = model.predict(X_test).flatten()\n",
    "actual = y_train[id_]\n",
    "\n",
    "# Plot the predictions.\n",
    "plt.plot(actual, \"blue\")\n",
    "plt.plot(pred, \"orange\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation sample.\n",
    "\n",
    "id_ = 30\n",
    "\n",
    "X_test = np.expand_dims(X_val[id_], axis = 0)\n",
    "\n",
    "pred = model.predict(X_test).flatten()\n",
    "actual = y_val[id_]\n",
    "\n",
    "# Plot the predictions.\n",
    "plt.plot(actual, \"blue\")\n",
    "plt.plot(pred, \"orange\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
