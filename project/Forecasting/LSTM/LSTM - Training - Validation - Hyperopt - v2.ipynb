{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pietro\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of the gpu.\n",
    "GPU = [0]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(x) for x in GPU)  \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the python path to the folder containing some useful custom packages.\n",
    "import sys\n",
    "sys.path.insert(0, \"../../packages/\")\n",
    "from TsIP.TsIP import TsIP\n",
    "from tools import find_multiple_sets\n",
    "from LagsCreator.LagsCreator import LagsCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workspace.\n",
    "dir = \"./output\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "else:\n",
    "    shutil.rmtree(dir)           \n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = \"Yemen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA_FOLDER = \"../../Dataset time-series/data/\" + COUNTRY + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the training sets.\n",
    "train = pd.read_csv(PATH_TO_DATA_FOLDER + \"train_smooth.csv\", header = [0, 1], index_col = 0)\n",
    "train.index.name = \"Datetime\"\n",
    "train.index = pd.to_datetime(train.index)\n",
    "freq = \"D\"\n",
    "train.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the test sets.\n",
    "test = pd.read_csv(PATH_TO_DATA_FOLDER + \"test_target.csv\", header = [0, 1], index_col = 0)\n",
    "test.index.name = \"Datetime\"\n",
    "test.index = pd.to_datetime(test.index)\n",
    "freq = \"D\"\n",
    "test.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of the whole time-series of the fcs indicator.\n",
    "target = pd.read_csv(PATH_TO_DATA_FOLDER + \"all_target.csv\", header = [0, 1], index_col = 0)\n",
    "target.index.name = \"Datetime\"\n",
    "target.index = pd.to_datetime(target.index)\n",
    "freq = \"D\"\n",
    "target.index.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 30\n",
    "FREQ = train.index.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Abyan', 'Aden', 'Al Bayda', 'Al Dhale'e', 'Al Hudaydah', 'Al Jawf',\n",
       "       'Al Maharah', 'Al Mahwit', 'Amanat Al Asimah', 'Amran', 'Dhamar',\n",
       "       'Hajjah', 'Ibb', 'Lahj', 'Marib', 'Raymah', 'Sa'ada', 'Sana'a',\n",
       "       'Shabwah', 'Taizz'],\n",
       "      dtype='object', name='AdminStrata')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROVINCES = TRAIN.columns.get_level_values(0).unique()\n",
    "PROVINCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1 Month Anomaly (%) Rainfall', '3 Months Anomaly (%) Rainfall',\n",
       "       'Cereals and tubers', 'Exchange rate (USD/LCU)', 'FCS', 'Fatality',\n",
       "       'Lat', 'Lon', 'NDVI Anomaly', 'Population', 'Rainfall (mm)', 'Ramadan',\n",
       "       'rCSI'],\n",
       "      dtype='object', name='Indicator')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTORS = TRAIN.columns.get_level_values(1).unique()\n",
    "PREDICTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source transformation\n",
    "\n",
    "I decide to normalize the data among the provinces considering indicator by indicator and considering only the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "global SCALERS\n",
    "\n",
    "MIN = 0\n",
    "MAX = 1\n",
    "SCALERS = dict()\n",
    "def normalization(group, feature_range):\n",
    "    min_, max_ = feature_range\n",
    "    min_group = group.min().min()\n",
    "    max_group = group.max().max()\n",
    "    \n",
    "    # Normalization.\n",
    "    group_std = (group - min_group) / (max_group - min_group)\n",
    "    group_scaled = group_std * (max_ - min_) + min_\n",
    "\n",
    "    # Save the scalers for the various indicators.\n",
    "    SCALERS[group.name] = (min_group, max_group)\n",
    "\n",
    "    return group_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>AdminStrata</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Abyan</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Taizz</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indicator</th>\n",
       "      <th>1 Month Anomaly (%) Rainfall</th>\n",
       "      <th>3 Months Anomaly (%) Rainfall</th>\n",
       "      <th>Cereals and tubers</th>\n",
       "      <th>Exchange rate (USD/LCU)</th>\n",
       "      <th>FCS</th>\n",
       "      <th>Fatality</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>NDVI Anomaly</th>\n",
       "      <th>Population</th>\n",
       "      <th>...</th>\n",
       "      <th>Exchange rate (USD/LCU)</th>\n",
       "      <th>FCS</th>\n",
       "      <th>Fatality</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>NDVI Anomaly</th>\n",
       "      <th>Population</th>\n",
       "      <th>Rainfall (mm)</th>\n",
       "      <th>Ramadan</th>\n",
       "      <th>rCSI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-08-22</th>\n",
       "      <td>0.202614</td>\n",
       "      <td>0.327464</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>0.097113</td>\n",
       "      <td>0.398119</td>\n",
       "      <td>0.016014</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.203677</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100691</td>\n",
       "      <td>0.521481</td>\n",
       "      <td>0.124455</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.317287</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.430215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-23</th>\n",
       "      <td>0.201694</td>\n",
       "      <td>0.323172</td>\n",
       "      <td>0.115215</td>\n",
       "      <td>0.108255</td>\n",
       "      <td>0.416745</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.201767</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105181</td>\n",
       "      <td>0.505624</td>\n",
       "      <td>0.118668</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.314277</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.436639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.513277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-24</th>\n",
       "      <td>0.200218</td>\n",
       "      <td>0.317813</td>\n",
       "      <td>0.118301</td>\n",
       "      <td>0.112477</td>\n",
       "      <td>0.422543</td>\n",
       "      <td>0.014297</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.199870</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109271</td>\n",
       "      <td>0.494372</td>\n",
       "      <td>0.115789</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.311164</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.446466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-25</th>\n",
       "      <td>0.198491</td>\n",
       "      <td>0.312132</td>\n",
       "      <td>0.119918</td>\n",
       "      <td>0.113002</td>\n",
       "      <td>0.424258</td>\n",
       "      <td>0.015185</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.197950</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113148</td>\n",
       "      <td>0.486382</td>\n",
       "      <td>0.115893</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.307901</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.458051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-26</th>\n",
       "      <td>0.196724</td>\n",
       "      <td>0.306528</td>\n",
       "      <td>0.121119</td>\n",
       "      <td>0.112481</td>\n",
       "      <td>0.427209</td>\n",
       "      <td>0.016327</td>\n",
       "      <td>0.204339</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.195990</td>\n",
       "      <td>0.137715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116965</td>\n",
       "      <td>0.480822</td>\n",
       "      <td>0.118913</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>0.304470</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.470108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.512072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "AdminStrata                        Abyan                                \\\n",
       "Indicator   1 Month Anomaly (%) Rainfall 3 Months Anomaly (%) Rainfall   \n",
       "Datetime                                                                 \n",
       "2018-08-22                      0.202614                      0.327464   \n",
       "2018-08-23                      0.201694                      0.323172   \n",
       "2018-08-24                      0.200218                      0.317813   \n",
       "2018-08-25                      0.198491                      0.312132   \n",
       "2018-08-26                      0.196724                      0.306528   \n",
       "\n",
       "AdminStrata                                                                 \\\n",
       "Indicator   Cereals and tubers Exchange rate (USD/LCU)       FCS  Fatality   \n",
       "Datetime                                                                     \n",
       "2018-08-22            0.109379                0.097113  0.398119  0.016014   \n",
       "2018-08-23            0.115215                0.108255  0.416745  0.014270   \n",
       "2018-08-24            0.118301                0.112477  0.422543  0.014297   \n",
       "2018-08-25            0.119918                0.113002  0.424258  0.015185   \n",
       "2018-08-26            0.121119                0.112481  0.427209  0.016327   \n",
       "\n",
       "AdminStrata                                              ...  \\\n",
       "Indicator         Lat       Lon NDVI Anomaly Population  ...   \n",
       "Datetime                                                 ...   \n",
       "2018-08-22   0.204339  0.354998     0.203677   0.137715  ...   \n",
       "2018-08-23   0.204339  0.354998     0.201767   0.137715  ...   \n",
       "2018-08-24   0.204339  0.354998     0.199870   0.137715  ...   \n",
       "2018-08-25   0.204339  0.354998     0.197950   0.137715  ...   \n",
       "2018-08-26   0.204339  0.354998     0.195990   0.137715  ...   \n",
       "\n",
       "AdminStrata                   Taizz                                        \\\n",
       "Indicator   Exchange rate (USD/LCU)       FCS  Fatality      Lat      Lon   \n",
       "Datetime                                                                    \n",
       "2018-08-22                 0.100691  0.521481  0.124455  0.13656  0.07253   \n",
       "2018-08-23                 0.105181  0.505624  0.118668  0.13656  0.07253   \n",
       "2018-08-24                 0.109271  0.494372  0.115789  0.13656  0.07253   \n",
       "2018-08-25                 0.113148  0.486382  0.115893  0.13656  0.07253   \n",
       "2018-08-26                 0.116965  0.480822  0.118913  0.13656  0.07253   \n",
       "\n",
       "AdminStrata                                                          \n",
       "Indicator   NDVI Anomaly Population Rainfall (mm) Ramadan      rCSI  \n",
       "Datetime                                                             \n",
       "2018-08-22      0.317287   0.894478      0.430215     0.0  0.522625  \n",
       "2018-08-23      0.314277   0.894478      0.436639     0.0  0.513277  \n",
       "2018-08-24      0.311164   0.894478      0.446466     0.0  0.511028  \n",
       "2018-08-25      0.307901   0.894478      0.458051     0.0  0.511425  \n",
       "2018-08-26      0.304470   0.894478      0.470108     0.0  0.512072  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_NORMALIZED = TRAIN.groupby(axis = 1, level = 1).apply(lambda x: normalization(x, (MIN, MAX)))\n",
    "TRAIN_NORMALIZED.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time-series.\n",
    "#TsIP(TRAIN_NORMALIZED).interactive_plot_df(title = \"Training sets\", matplotlib = False, style = \"lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalization(group_scaled, indicator, feature_range, scalers):\n",
    "    min_, max_ = feature_range\n",
    "    min_group, max_group = scalers[indicator]\n",
    "\n",
    "    group_std = (group_scaled - min_) / (max_ - min_)\n",
    "    group = (group_std * (max_group - min_group)) + min_group\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test sets.\n",
    "TRAIN_NORMALIZED_SETS = find_multiple_sets(TRAIN_NORMALIZED)\n",
    "TEST_TARGET_SETS = find_multiple_sets(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation\n",
    "### Parameters grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the PARAMETERS MODEL to which perform the grid search.\n",
    "space = {\"lags\": hp.choice(\"lags\", np.arange(1, 100, 5)), \n",
    "         \"batch_size\": 512,#hp.choice(\"batch_size\", np.array([2**j for j in range(3, 10)])), \n",
    "         \"LSTM\": hp.randint(\"LSTM\", 1, 100), \n",
    "         \"Conv1\": hp.randint(\"Conv1\", 1, 64),\n",
    "         \"Conv2\": hp.randint(\"Conv2\", 1, 5),\n",
    "         \"Dense\": hp.randint(\"Dense\", 1, 100), \n",
    "         \"Dropout\": hp.uniform(\"Dropout\", 0, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(timesteps, features, n_out, lstm, conv1, conv2, dense, dropout):\n",
    "    \n",
    "    inp_seq = Input(shape = (timesteps, features))\n",
    "    \n",
    "    x = Bidirectional(LSTM(lstm, return_sequences = True))(inp_seq)\n",
    "    x = AveragePooling1D(2)(x)\n",
    "    x = Conv1D(conv1, conv2, activation = \"relu\", padding = \"same\", name = \"extractor\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(dense, activation = \"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    out = Dense(n_out)(x)\n",
    "    \n",
    "    model = Model(inp_seq, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameters(space):      \n",
    "    #try:\n",
    "    # Define the parameters to grid search.\n",
    "    LAGS = int(space[\"lags\"])\n",
    "    BATCH_SIZE = int(space[\"batch_size\"])\n",
    "    lstm = int(space[\"LSTM\"])\n",
    "    conv1 = int(space[\"Conv1\"])\n",
    "    conv2 = int(space[\"Conv2\"])\n",
    "    dense = int(space[\"Dense\"])\n",
    "    dropout = space[\"Dropout\"]\n",
    "\n",
    "    lags_dict = dict()\n",
    "    # Define lags for each indicator.\n",
    "    lags_dict[\"3 Months Anomaly (%) Rainfall\"] = LAGS\n",
    "    lags_dict[\"1 Month Anomaly (%) Rainfall\"] = LAGS\n",
    "    lags_dict[\"Cereals and tubers\"] = LAGS\n",
    "    lags_dict[\"Exchange rate (USD/LCU)\"] = LAGS\n",
    "    lags_dict[\"FCS\"] = LAGS\n",
    "    lags_dict[\"Fatality\"] = LAGS\n",
    "    lags_dict[\"NDVI Anomaly\"] = LAGS\n",
    "    lags_dict[\"Rainfall (mm)\"] = LAGS\n",
    "    lags_dict[\"rCSI\"] = LAGS\n",
    "    lags_dict[\"Lat\"] = LAGS\n",
    "    lags_dict[\"Lon\"] = LAGS\n",
    "    lags_dict[\"Population\"] = LAGS\n",
    "    lags_dict[\"Ramadan\"] = LAGS\n",
    "    \n",
    "    # Randomly select only some predictors.\n",
    "    #predictors = list(np.random.choice(PREDICTORS, size = np.random.randint(len(PREDICTORS) + 1), replace = False))\n",
    "    #if \"FCS\" not in predictors:\n",
    "    #    predictors.append(\"FCS\")      \n",
    "    #for k,v in lags_dict.items():\n",
    "    #    if k not in predictors:\n",
    "    #        lags_dict[k] = None\n",
    "            \n",
    "    #print(\"lags: %d, batch_size: %d, n_predictors: %d\" %(LAGS, BATCH_SIZE, len(predictors)))\n",
    "            \n",
    "    X_train_list, y_train_list, X_val_list, y_val_list = list(), list(), list(), list()\n",
    "    # Create training and validation points starting from the training sets.\n",
    "    for train_normalized in TRAIN_NORMALIZED_SETS:\n",
    "        # Create training points and validation points from the training set.\n",
    "        for PROVINCE in PROVINCES:\n",
    "            # Initialize lags creator.\n",
    "            creator = LagsCreator(train_normalized[[PROVINCE]], lags_dictionary = lags_dict, target = \"FCS\")\n",
    "            # Get samples.\n",
    "            X_train, y_train, X_val, y_val, _ = creator.to_supervised(n_out = TEST_SIZE, single_step = False, return_dataframe = False, \n",
    "                                                                      feature_time = False, validation = True, dtype = np.float32)\n",
    "\n",
    "            # Add a list of all the training and validation samples of all the provinces together.\n",
    "            X_train_list.append(X_train)\n",
    "            y_train_list.append(y_train)\n",
    "            X_val_list.append(X_val)\n",
    "            y_val_list.append(y_val)\n",
    "\n",
    "    X_train = np.concatenate(X_train_list)\n",
    "    y_train = np.concatenate(y_train_list)\n",
    "    X_val = np.concatenate(X_val_list)\n",
    "    y_val = np.concatenate(y_val_list)\n",
    "\n",
    "    N_FEATURES = X_train.shape[2]\n",
    "\n",
    "    # Model.\n",
    "    model = network(LAGS, N_FEATURES, TEST_SIZE, lstm, conv1, conv2, dense, dropout)\n",
    "    # Compile model.\n",
    "    model.compile(loss = \"mse\", optimizer = \"adam\")\n",
    "\n",
    "    # Patient early stopping.\n",
    "    es = EarlyStopping(monitor = \"val_loss\", mode = \"min\", verbose = 1, patience = 30)\n",
    "    # Fit model.\n",
    "    history = model.fit(X_train, y_train, epochs = N_EPOCHS, validation_data = (X_val, y_val), \n",
    "                        batch_size = BATCH_SIZE, verbose = 0, shuffle = True, callbacks = [es])\n",
    "    \n",
    "    # Save the number of epochs at which fit stop due to early stopping.\n",
    "    number_of_epochs_it_ran = len(history.history[\"loss\"])  \n",
    "    train_loss = history.history[\"loss\"][-1]\n",
    "    val_loss = history.history[\"val_loss\"][-1]\n",
    "\n",
    "    # Recursive save results.\n",
    "    results = space.copy()\n",
    "    results.update(lags_dict)\n",
    "    results[\"epoch\"] = number_of_epochs_it_ran\n",
    "    results[\"val_loss\"] = val_loss\n",
    "    results[\"train_loss\"] = train_loss\n",
    "    df_space = pd.DataFrame(results, index = [0], dtype = object)\n",
    "    filename = dir + \"/grid_search.csv\"\n",
    "    df_space.to_csv(filename, index = False, header = (not os.path.exists(filename)), mode = \"a\")\n",
    "    \n",
    "    best_val_loss = pd.read_csv(dir + \"/grid_search.csv\")\n",
    "    best_val_loss = best_val_loss.iloc[best_val_loss.val_loss.idxmin()].val_loss\n",
    "    \n",
    "    if val_loss <= best_val_loss:\n",
    "        model.save(dir + \"/best_model.h5\")\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    #except:\n",
    "    #    val_loss = np.inf     \n",
    "    #    K.clear_session()\n",
    "\n",
    "    return {\"loss\": val_loss, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 10/10 [01:13<00:00,  7.36s/trial, best loss: 0.09293047338724136]\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = hyperparameters,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 10,\n",
    "            trials = trials)\n",
    "\n",
    "# Save the trials into a file.\n",
    "pickle.dump(trials, open(dir + \"/hyp_trials.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
